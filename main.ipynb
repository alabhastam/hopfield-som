{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14199917,"sourceType":"datasetVersion","datasetId":9055859}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"ed82f81f","cell_type":"markdown","source":"# Deep Learning Assignment: Self-Organizing Maps and Hopfield Networks\n---","metadata":{}},{"id":"7caa3330","cell_type":"markdown","source":"# Section A: Self-Organizing Maps","metadata":{}},{"id":"5af27f07","cell_type":"markdown","source":"**Part 1: Setup and Imports**\n\nImport necessary libraries, set the seed for reproducability, load the Iris dataset and scale it.","metadata":{}},{"id":"1c38387a","cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import Normalize\n\nfrom sklearn import datasets\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\nnp.random.seed(42)\n\nplt.rcParams[\"figure.figsize\"] = (6, 5)\nplt.rcParams[\"axes.grid\"] = True\nplt.rcParams[\"axes.spines.top\"] = False\nplt.rcParams[\"axes.spines.right\"] = False\n\niris = datasets.load_iris()\nX = iris.data  # (150, 4)\ny = iris.target  # labels: 0, 1, 2\n\n# Standardize features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# PCA for 2D visualization\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled)\n\nprint(\"X_scaled shape:\", X_scaled.shape)\nprint(\"X_pca shape:\", X_pca.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T19:33:01.843386Z","iopub.execute_input":"2025-12-17T19:33:01.843786Z","iopub.status.idle":"2025-12-17T19:33:06.310979Z","shell.execute_reply.started":"2025-12-17T19:33:01.843753Z","shell.execute_reply":"2025-12-17T19:33:06.309744Z"}},"outputs":[{"name":"stdout","text":"X_scaled shape: (150, 4)\nX_pca shape: (150, 2)\n","output_type":"stream"}],"execution_count":1},{"id":"df764ae0","cell_type":"markdown","source":"**Part 2: Implementation of a Self-Organizing Map**\n\n**TODO**: Complete the SOM class implementation below.","metadata":{}},{"id":"a832b0ab","cell_type":"code","source":"class SOM:\n    \"\"\"\n    2D Self-Organizing Map with a square lattice neighborhood.\n\n    The lattie is a square 2D grid (m, n):\n      - grid coordinates (i, j)\n      - grid distance: d^2 = (i - i_bmu)^2 + (j - j_bmu)^2\n    \"\"\"\n\n    def __init__(self, m, n, dim, lr0=0.5, sigma0=None, n_epochs=50):\n        self.m = m\n        self.n = n\n        self.dim = dim\n        self.lr0 = lr0\n        self.n_epochs = n_epochs\n        self.sigma0 = sigma0 if sigma0 is not None else max(m, n) / 2.0\n\n        # Regular 2D grid coordinates\n        self.gx, self.gy = np.meshgrid(np.arange(m), np.arange(n), indexing=\"ij\")\n\n        # TODO: Initialize weight vectors of shape (m, n, dim) following standard normal distribution\n        pass\n\n    def _learning_rate(self, epoch):\n        \"\"\"\n        Exponential decay learning rate schedule.\n        \"\"\"\n        # TODO: Implement exponential decay for learning rate\n        pass\n\n    def _sigma(self, epoch):\n        \"\"\"\n        Exponential decay for neighborhood radius.\n        \"\"\"\n        # TODO: Implement exponential decay for neighborhood radius\n        pass\n\n    def find_bmu(self, x):\n        \"\"\"\n        Find Best Matching Unit (BMU) for input x.\n        The BMU is the neuron closest to the given input.\n        Returns:\n            (i, j): indices of the BMU in the (m, n) grid.\n        \"\"\"\n        # TODO: Compute squared Euclidean distance to all weights and\n        #       return index of minimum.\n        pass\n\n    def _neighborhood(self, bmu, sigma):\n        \"\"\"\n        Gaussian neighborhood on a square grid.\n\n        Args:\n            bmu   : (i, j) index of BMU\n            sigma : neighborhood radius\n\n        Returns:\n            h     : (m, n) array of neighborhood values\n        \"\"\"\n        # TODO: Implement Gaussian neighborhood function:\n        #   - Compute dx, dy := differences to all grid points\n        #   - d^2 = dx^2 + dy^2\n        #   - h = exp( - d^2 / (2 * sigma^2) )\n        pass\n\n    def train(self, data):\n        \"\"\"\n        Train SOM on the given data with shape: (N, dim).\n\n        High-level algorithm for each epoch:\n            1. Compute current learning rate eta = self._learning_rate(epoch)\n            2. Compute current neighborhood radius sigma = self._sigma(epoch)\n            3. Shuffle the data indices\n            4. For each sample x in the shuffled data:\n                a) Find the Best Matching Unit (BMU) for x\n                b) Compute the neighborhood function h around the BMU\n                c) Update ALL weights using:\n                        w_ij <- w_ij + eta * h_ij * (x - w_ij)\n        \"\"\"\n        # TODO: Implement the training loop\n        pass\n\n    def map_vects(self, data):\n        \"\"\"\n        Map each input vector to its BMU.\n        Returns:\n            bmus: (N, 2) array of (i, j) indices\n        \"\"\"\n        # TODO: For each x in data, find BMU and store (i, j).\n        pass","metadata":{},"outputs":[],"execution_count":null},{"id":"cd8cb8ff","cell_type":"markdown","source":"**Part 3: Train the SOM**\n\nCreate an instance of SOM and train it on the Iris dataset. Extract weights of learned neurons and apply PCA.","metadata":{}},{"id":"a800783d","cell_type":"code","source":"x_neurons, y_neurons = 10, 10\nsom = SOM(m=x_neurons, n=y_neurons, dim=X_scaled.shape[1], lr0=0.5, n_epochs=60)\nsom.train(X_scaled)\n\nweights_flat = som.weights.reshape(-1, X_scaled.shape[1])\nweights_pca = pca.transform(weights_flat)\n\nprint(\"SOM trained.\")","metadata":{},"outputs":[],"execution_count":null},{"id":"2d14e3ef","cell_type":"markdown","source":"**Part 4: Topology Visualization**\n\nVisualize the learned topology in PCA space.","metadata":{}},{"id":"181efd5e","cell_type":"code","source":"fig, ax = plt.subplots(figsize=(6, 5))\n\n# Iris data in PCA space\nscatter = ax.scatter(\n    X_pca[:, 0], X_pca[:, 1], c=y, cmap=\"viridis\", s=30, alpha=0.5, label=\"Iris data\"\n)\n\n# Neuron positions in PCA space\nW2 = weights_pca.reshape(x_neurons, y_neurons, 2)\n\ndirections = [(-1, 0), (1, 0), (0, -1), (0, 1)]\nfor i in range(x_neurons):\n    for j in range(y_neurons):\n        x_ij, y_ij = W2[i, j]\n        for di, dj in directions:\n            ni, nj = i + di, j + dj\n            if 0 <= ni < x_neurons and 0 <= nj < y_neurons:\n                x_n, y_n = W2[ni, nj]\n                ax.plot([x_ij, x_n], [y_ij, y_n], \"-k\", linewidth=0.4, alpha=0.7)\n\n# Plot neuron positions\nax.scatter(W2[:, :, 0], W2[:, :, 1], c=\"red\", s=25, label=\"SOM neurons\")\n\nax.set_title(\"SOM topology in PCA space of Iris dataset\")\nax.set_xlabel(\"PC1\")\nax.set_ylabel(\"PC2\")\nax.legend(loc=\"best\")\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"id":"da340cf0","cell_type":"markdown","source":"**Part 5: U-Matrix**\n\nThe U-matrix measures, for each neuron, the average distance to its neighbors. High U-values often correspond to _cluster boundaries_.\n\n**TODO**: Complete the implementation of `compute_u_matrix` method below.","metadata":{}},{"id":"d1e19c9e","cell_type":"code","source":"def compute_u_matrix(weights):\n    \"\"\"\n    Compute a U-matrix for a square-lattice SOM.\n\n    U[i, j] = average distance between neuron (i, j) and its\n              4 neighbors (up/down/left/right), where they exist:\n\n        (i-1, j)  # up\n        (i+1, j)  # down\n        (i, j-1)  # left\n        (i, j+1)  # right\n\n    Args:\n        weights : (m, n, dim) array\n\n    Returns:\n        U       : (m, n) array\n    \"\"\"\n    m, n, _ = weights.shape\n    U = np.zeros((m, n), dtype=float)\n\n    directions = [\n        (-1, 0),\n        (1, 0),\n        (0, -1),\n        (0, 1),\n    ]\n\n    # TODO: For each neuron at (i, j) compute its Euclidean distance to its 4-neighbors\n    #       and store the average in U[i,j]\n    pass\n\n\nU = compute_u_matrix(som.weights)\nprint(\"U-matrix shape:\", U.shape)","metadata":{},"outputs":[],"execution_count":null},{"id":"0fcaa356","cell_type":"markdown","source":"**Part 6: U-matrix and average distance plot**\n\nPlot the calculated U-matrix as well as the learned topology where each neuron's color depends on the average distance between its neighbors: The higher the average distance, the lighter the color of the neuron.\n\n**TODO**: Run the next cell and write a short interpretation of the plots.","metadata":{}},{"id":"080bb85d","cell_type":"code","source":"grid_m, grid_n = som.m, som.n\n\nW2 = weights_pca.reshape(grid_m, grid_n, 2)\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\nax = axes[0]\nim = ax.imshow(U, cmap=\"viridis\", origin=\"upper\")\ncbar = plt.colorbar(im, ax=ax)\ncbar.set_label(\"Average neighbor distance\")\n\nax.set_title(\"U-matrix\")\nax.set_xlabel(\"Grid column (j)\")\nax.set_ylabel(\"Grid row (i)\")\n\n\nax = axes[1]\nax.scatter(X_pca[:, 0], X_pca[:, 1], c=\"lightgrey\", s=20, alpha=0.6, label=\"Iris data\")\n\ndirections = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n\n\nfor i in range(grid_m):\n    for j in range(grid_n):\n        x_ij, y_ij = W2[i, j]\n        for di, dj in directions:\n            ni, nj = i + di, j + dj\n            if 0 <= ni < grid_m and 0 <= nj < grid_n:\n                x_n, y_n = W2[ni, nj]\n                ax.plot([x_ij, x_n], [y_ij, y_n], \"-k\", linewidth=0.3, alpha=0.5)\n\n\nnorm = Normalize(vmin=U.min(), vmax=U.max())\ncmap = plt.cm.viridis\n\nU_flat = U.flatten()\ncolors_flat = cmap(norm(U_flat))\n\nax.scatter(\n    W2[:, :, 0].ravel(),\n    W2[:, :, 1].ravel(),\n    c=colors_flat,\n    s=30,\n    edgecolor=\"k\",\n    linewidth=0.3,\n    label=\"SOM neurons\",\n)\n\nax.set_title(\"SOM topology in PCA space\")\nax.set_xlabel(\"PC1\")\nax.set_ylabel(\"PC2\")\n\nplt.tight_layout()\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"id":"177d878e","cell_type":"markdown","source":"**part 7: Quantization error**\n\nQuantization error is the average distance between each input to its BMU. Below, we run train SOM with 5 different grid sizes and print the quantization error.\n\n**TODO**: What trend do you observe in the printed values? Explain why this happens.","metadata":{}},{"id":"e933e97b","cell_type":"code","source":"def quantization_error(data, som):\n    total_dist = 0.0\n    for x in data:\n        i, j = som.find_bmu(x)\n        w = som.weights[i, j]\n        total_dist += np.linalg.norm(x - w)\n    return total_dist / data.shape[0]\n\n\nnp.random.seed(42)\ntopology_config = [(3, 3), (5, 5), (8, 8), (10, 10), (15, 15)]\nfor m, n in topology_config:\n    net = SOM(m=m, n=n, dim=X_scaled.shape[1], lr0=0.5, n_epochs=60)\n    net.train(X_scaled)\n    qe = quantization_error(X_scaled, net)\n    print(f\"Quantization error for config {(x_neurons, y_neurons)}: {qe:.4f}\")","metadata":{},"outputs":[],"execution_count":null},{"id":"a3917083","cell_type":"markdown","source":"# Section B: Hopfield Networks","metadata":{}},{"id":"d0a06fe3","cell_type":"markdown","source":"**Part 1: Plotting functions**","metadata":{}},{"id":"cf94d9d8","cell_type":"code","source":"np.random.seed(42)\n\n\ndef plot_pattern(pattern, title=None, ax=None):\n    \"\"\"\n    Visualize a single 10x10 bipolar pattern.\n    \"\"\"\n    if ax is None:\n        _, ax = plt.subplots()\n    img = (pattern.reshape(10, 10) + 1) / 2.0\n    ax.imshow(img, cmap=\"gray_r\", interpolation=\"nearest\")\n    ax.axis(\"off\")\n    if title is not None:\n        ax.set_title(title)\n\n\ndef plot_pattern_triplet(original, corrupted, recalled, titles=None):\n    if titles is None:\n        titles = [\"Original\", \"Corrupted\", \"Recalled\"]\n    _, axes = plt.subplots(1, 3, figsize=(8, 3))\n    plot_pattern(original, title=titles[0], ax=axes[0])\n    plot_pattern(corrupted, title=titles[1], ax=axes[1])\n    plot_pattern(recalled, title=titles[2], ax=axes[2])\n    plt.tight_layout()\n    plt.show()","metadata":{},"outputs":[],"execution_count":null},{"id":"fb3f30e7","cell_type":"markdown","source":"**Part 2: Digit Visualization**\n\nPlot the 3 digits 0, 1, and 8.","metadata":{}},{"id":"876dc376","cell_type":"code","source":"digit_patterns = np.load(\"digits.npy\")  # shape (3,100)\ndigit_labels = [\"0\", \"1\", \"8\"]\n\nfig, axes = plt.subplots(1, 3, figsize=(9, 3))\nfor i, lbl in enumerate(digit_labels):\n    plot_pattern(digit_patterns[i], title=lbl, ax=axes[i])\nplt.tight_layout()\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"id":"d1d873b3","cell_type":"markdown","source":"**Part 3: Weight Matrix Calculation**\n\n**TODO**: Complete the implementation of `compute_weights`.","metadata":{}},{"id":"9fea0fdb","cell_type":"code","source":"def compute_weights(patterns):\n    \"\"\"\n    patterns has shape shape (P, N) with bipolar values\n    returns: W (N,N)\n    \"\"\"\n    # TODO: Implement the weight matrix (remember to set the diagonal to zero)\n    pass\n\n\nW = compute_weights(digit_patterns)\nprint(\"W shape:\", W.shape)","metadata":{},"outputs":[],"execution_count":null},{"id":"acc2811d","cell_type":"markdown","source":"**Part 4: Update Rules**\n\nHopfield networks could be updated in two different manners: _Synchronous_ and _Asynchronous_.\n\n**TODO**: Complete the implementation of `sgn`, `update_sync` and `update_async` methods below.","metadata":{}},{"id":"daaf15b4","cell_type":"code","source":"def sgn(x):\n    # TODO: Implement the sign() function\n    pass\n\n\ndef update_sync(x0, W, max_steps=50):\n    \"\"\"\n    Synchronous update rule for Hopfield network.\n\n    x0: initial state (N,) with values in {-1, +1}\n    W: weight matrix (N, N)\n    max_steps: maximum number of synchronous update steps\n\n    Returns:\n        traj: list of states (each is an array of shape (N,))\n              including the initial state as traj[0].\n    \"\"\"\n    # TODO\n    pass\n\n\ndef update_async(x0, W, max_sweeps=50):\n    \"\"\"\n    Asynchronous update rule for Hopfield network.\n\n    x0: initial state (N,) with values in {-1, +1}\n    W: weight matrix (N, N)\n    max_steps: maximum number of updates over all neurons\n\n    Returns:\n        traj: list of states (each is an array of shape (N,))\n              including the initial state as traj[0].\n    \"\"\"\n    # TODO\n    pass","metadata":{},"outputs":[],"execution_count":null},{"id":"526ffcfe","cell_type":"markdown","source":"**Part 5: Recall Experiment**\n\nApply 3 types of noise to the stored patterns: a) random bit flipping, b) erasing the bottom half of the digit, and c) erasing the right half of the digit. Then, use the Hopfield network to recall a pattern.\n\n**TODO**: In which cases has the network recalled the correct patterns and in which cases has it failed?","metadata":{}},{"id":"6a3f75ab","cell_type":"code","source":"def flip_bits(x, p):\n    mask = np.random.rand(len(x)) < p\n    x2 = x.copy()\n    x2[mask] *= -1\n    return x2\n\n\nselected_digits = [0, 1, 2]  # corresponds to indices of [\"0\",\"1\",\"8\"]\nnoise_types = [\"bitflip\", \"bottom\", \"right\"]\n\nfig, axes = plt.subplots(\n    len(selected_digits) * 3, 2, figsize=(6, 2.2 * len(selected_digits) * 3)\n)\n\nrow = 0\nfor idx in selected_digits:\n    x_orig = digit_patterns[idx]\n    label = digit_labels[idx]\n\n    for noise in noise_types:\n        if noise == \"bitflip\":\n            noisy = flip_bits(x_orig, p=0.05)\n\n        elif noise == \"bottom\":\n            img = x_orig.reshape(10, 10).copy()\n            img[5:, :] = -1\n            noisy = img.reshape(-1)\n\n        elif noise == \"right\":\n            img = x_orig.reshape(10, 10).copy()\n            img[:, 5:] = -1\n            noisy = img.reshape(-1)\n\n        traj = update_async(noisy, W, max_sweeps=50)\n        recalled = traj[-1]\n\n        axL = axes[row, 0]\n        axR = axes[row, 1]\n\n        plot_pattern(noisy, title=f\"{label} ({noise}) noisy\", ax=axL)\n        plot_pattern(recalled, title=f\"Recalled\", ax=axR)\n\n        row += 1\n\nplt.tight_layout()\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"id":"41cd4e9a","cell_type":"markdown","source":"**Part 6: Energy of States**\n\nEach stored pattern in a Hopfield network has an energy. The lower the energy of a state, the stronger of an attractor it is.\n\n**TODO**: Complete the imlpementation of `energy`.","metadata":{}},{"id":"192fff49","cell_type":"code","source":"def energy(x, W):\n    # TODO: Compute the energy of state x for a given weight matrix W\n    pass\n\n\nfor i in range(3):\n    print(f\"Energy of digit {digit_labels[i]}:, {energy(digit_patterns[i], W):.2f}\")","metadata":{},"outputs":[],"execution_count":null},{"id":"241da680","cell_type":"markdown","source":"**Part 7: Spurious Patterns**\n\nNow, we are going to find (almost) all of the spurious patterns stored inside the Hopfield network.\n\n**TODO**: Run the cell below and answer the questions:\n- Are there any spurious patterns?\n- Compare the energy of stored and spurious patterns.\n- Explain why some of the noisy digits in **part 6** were recalled uncorrectly (if any).","metadata":{}},{"id":"c6f1499e","cell_type":"code","source":"def discover_attractors(W, max_samples=5000, patience=500, max_steps=50):\n    N = W.shape[0]\n    attractors = []\n    no_new_count = 0\n\n    for _ in range(max_samples):\n        x0 = np.random.choice([-1, 1], size=N)\n        x_final = update_async(x0, W, max_sweeps=max_steps)[-1]\n\n        is_new = True\n        for a in attractors:\n            if np.array_equal(a, x_final):\n                is_new = False\n                break\n\n        if is_new:\n            attractors.append(x_final.copy())\n            no_new_count = 0\n        else:\n            no_new_count += 1\n\n        if no_new_count >= patience:\n            break\n\n    return attractors\n\n\nattractors = discover_attractors(W, max_samples=5000, patience=500, max_steps=50)\nprint(f\"Discovered {len(attractors)} distinct attractors.\")\n\n\ndef classify_attractor(a, patterns, labels):\n    for i in range(patterns.shape[0]):\n        if np.array_equal(a, patterns[i]):\n            return (\"stored\", labels[i])\n        if np.array_equal(a, -patterns[i]):\n            return (\"neg_stored\", labels[i])\n    return (\"spurious\", None)\n\n\nclassified = [classify_attractor(a, digit_patterns, digit_labels) for a in attractors]\n\nfor idx, a in enumerate(attractors):\n    kind, lab = classified[idx]\n    E = energy(a, W)\n    if lab is None:\n        print(f\"Attractor {idx}: {kind}, E = {E:.3f}\")\n    else:\n        print(f\"Attractor {idx}: {kind} ({lab}), E = {E:.3f}\")\n\n\nstored_indices = [i for i, c in enumerate(classified) if c[0] == \"stored\"]\nneg_stored_indices = [i for i, c in enumerate(classified) if c[0] == \"neg_stored\"]\nspurious_indices = [i for i, c in enumerate(classified) if c[0] == \"spurious\"]\n\n\ndef show_attractor_group(indices, title_prefix):\n    if not indices:\n        print(f\"No {title_prefix.lower()} attractors found.\")\n        return\n    n = len(indices)\n    _, axes = plt.subplots(1, n, figsize=(3 * n, 3))\n    if n == 1:\n        axes = [axes]\n    for k, idx in enumerate(indices):\n        a = attractors[idx]\n        kind, lab = classified[idx]\n        E = energy(a, W)\n\n        if lab is None:\n            t = f\"{title_prefix} {idx}\\nE = {E:.2f}\"\n        else:\n            t = f\"{title_prefix} {idx} ({lab})\\nE = {E:.2f}\"\n\n        plot_pattern(a, title=t, ax=axes[k])\n    plt.tight_layout()\n    plt.show()\n\n\n# Original patterns as attractors\nshow_attractor_group(stored_indices, \"Stored\")\n\n# Negated stored patterns\nshow_attractor_group(neg_stored_indices, \"Neg-stored\")\n\n# Spurious patterns\nshow_attractor_group(spurious_indices, \"Spurious\")","metadata":{},"outputs":[],"execution_count":null}]}